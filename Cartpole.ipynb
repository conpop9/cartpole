{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXJcMpxLCxqUJLmagi4Kn1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/conpop9/cartpole/blob/main/Cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSufO6ky1E7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c04c999-296a-4f7b-f290-3184594a035d"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "import gym \n",
        "\n",
        "import time \n",
        "\n",
        "import math \n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "print(env.action_space.n)\n",
        "\n",
        "LEARNING_RATE = 0.1\n",
        "\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 50000\n",
        "total = 0\n",
        "total_reward = 0\n",
        "prior_reward = 0\n",
        "\n",
        "Observation = [30, 30, 50, 50]\n",
        "np_array_win_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
        "\n",
        "epsilon = 1\n",
        "\n",
        "epsilon_decay_value = 0.99995\n",
        "\n",
        "q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
        "q_table.shape\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = state/np_array_win_size+ np.array([15,10,1,10])\n",
        "    return tuple(discrete_state.astype(np.int))\n",
        "\n",
        "\n",
        "for episode in range(EPISODES + 1): \n",
        "    t0 = time.time() \n",
        "    discrete_state = get_discrete_state(env.reset())  \n",
        "    done = False\n",
        "    episode_reward = 0 \n",
        "\n",
        "    if episode % 1000 == 0: \n",
        "        print(\"Episode: \" + str(episode))\n",
        "\n",
        "    while not done: \n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "\n",
        "            action = np.argmax(q_table[discrete_state]) \n",
        "        else:\n",
        "\n",
        "            action = np.random.randint(0, env.action_space.n) \n",
        "\n",
        "        new_state, reward, done, _ = env.step(action) \n",
        "\n",
        "        episode_reward += reward \n",
        "\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % 2000 == 0: \n",
        "            env.render\n",
        "\n",
        "        if not done: \n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    if epsilon > 0.05: \n",
        "        if episode_reward > prior_reward and episode > 10000:\n",
        "            epsilon = math.pow(epsilon_decay_value, episode - 10000)\n",
        "\n",
        "            if episode % 500 == 0:\n",
        "                print(\"Epsilon: \" + str(epsilon))\n",
        "\n",
        "    t1 = time.time() \n",
        "    episode_total = t1 - t0 \n",
        "    total = total + episode_total\n",
        "\n",
        "    total_reward += episode_reward \n",
        "    prior_reward = episode_reward\n",
        "\n",
        "    if episode % 1000 == 0:\n",
        "        mean = total / 1000\n",
        "        print(\"Time Average: \" + str(mean))\n",
        "        total = 0\n",
        "\n",
        "        mean_reward = total_reward / 1000\n",
        "        print(\"Mean Reward: \" + str(mean_reward))\n",
        "        total_reward = 0\n",
        "\n",
        "env.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "Episode: 0\n",
            "Time Average: 2.2072792053222654e-06\n",
            "Mean Reward: 0.03\n",
            "Time Average: 0.0009426186084747314\n",
            "Mean Reward: 21.811\n",
            "Episode: 2000\n",
            "Time Average: 0.0009127695560455322\n",
            "Mean Reward: 21.939\n",
            "Time Average: 0.0009073078632354737\n",
            "Mean Reward: 21.697\n",
            "Episode: 4000\n",
            "Time Average: 0.0009207916259765625\n",
            "Mean Reward: 22.077\n",
            "Time Average: 0.0009185750484466553\n",
            "Mean Reward: 22.209\n",
            "Episode: 6000\n",
            "Time Average: 0.0009478754997253418\n",
            "Mean Reward: 22.387\n",
            "Time Average: 0.0009517481327056884\n",
            "Mean Reward: 22.534\n",
            "Episode: 8000\n",
            "Time Average: 0.0009321236610412598\n",
            "Mean Reward: 22.339\n",
            "Time Average: 0.000922184944152832\n",
            "Mean Reward: 22.084\n",
            "Episode: 10000\n",
            "Time Average: 0.0009341192245483398\n",
            "Mean Reward: 22.302\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}